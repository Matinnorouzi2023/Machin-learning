{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Matinnorouzi2023/Machin-learning/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7ImPeIoHIEV",
        "outputId": "78bf2207-7d42-48d9-83cc-ab968ef08f3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CMU-MultimodalSDK'...\n",
            "remote: Enumerating objects: 100, done.\u001b[K\n",
            "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
            "remote: Compressing objects: 100% (82/82), done.\u001b[K\n",
            "remote: Total 100 (delta 13), reused 95 (delta 11), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (100/100), 294.05 KiB | 1.78 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/CMU-MultiComp-Lab/CMU-MultimodalSDK.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlNYjuKuRE1Q",
        "outputId": "c7319e33-df68-4e2a-9257-8dec1c39ce09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CMU-MultimodalSDK\n"
          ]
        }
      ],
      "source": [
        "cd CMU-MultimodalSDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln05YtkDRIte",
        "outputId": "d04286a2-8600-4350-d9e7-5c63a53fa9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.12.1)\n",
            "Collecting validators>=0.22.0 (from -r requirements.txt (line 3))\n",
            "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.67.1)\n",
            "Collecting colorama>=0.4.6 (from -r requirements.txt (line 5))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: validators, colorama\n",
            "Successfully installed colorama-0.4.6 validators-0.34.0\n"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "kfE9WjZvROdh",
        "outputId": "5dc963b3-6715-409b-978c-3f2d01954230"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/CMU-MultimodalSDK\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from mmsdk==1.1.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from mmsdk==1.1.0) (3.12.1)\n",
            "Requirement already satisfied: validators>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from mmsdk==1.1.0) (0.34.0)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from mmsdk==1.1.0) (4.67.1)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from mmsdk==1.1.0) (0.4.6)\n",
            "Building wheels for collected packages: mmsdk\n",
            "  Building wheel for mmsdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmsdk: filename=mmsdk-1.1.0-py3-none-any.whl size=79210 sha256=931c3cde70e038aa975f6807297dac7e2a3965d27094b3069f19f8e07bde683d\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/67/7d/7051855c4c5b9ee1d6fd0623588d84f217296da3b33953a1ab\n",
            "Successfully built mmsdk\n",
            "Installing collected packages: mmsdk\n",
            "  Attempting uninstall: mmsdk\n",
            "    Found existing installation: mmsdk 1.1.0\n",
            "    Uninstalling mmsdk-1.1.0:\n",
            "      Successfully uninstalled mmsdk-1.1.0\n",
            "Successfully installed mmsdk-1.1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mmsdk"
                ]
              },
              "id": "759aad96fd5a4cf8993e8d5ce87c96a6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85aNuHaKRSTz",
        "outputId": "474426b4-4a66-4642-ce86-124d4779338f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CMU-MultimodalSDK نصب شد!\n"
          ]
        }
      ],
      "source": [
        "import mmsdk\n",
        "print(\"CMU-MultimodalSDK نصب شد!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "lLw4RLiJRVF7",
        "outputId": "a6ae3367-73a4-465e-d520-ebbb62a9c8c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[91m\u001b[1m[2025-01-17 19:02:36.526] | Error   | \u001b[0mcmumosi/CMU_MOSI_TimestampedWordVectors.csd file already exists ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "cmumosi/CMU_MOSI_TimestampedWordVectors.csd file already exists ...",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-3fcfcf4140d3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmmsdk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmmdatasdk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcmumosi_highlevel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmmdatasdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmmdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmdatasdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmu_mosi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhighlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cmumosi/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcmumosi_highlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove_vectors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollapse_functions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmyavg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcmumosi_highlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_computational_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmmdatasdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmu_mosi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cmumosi/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcmumosi_highlevel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Opinion Segment Labels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/dataset/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, recipe, destination)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputational_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomputational_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecipe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/computational_sequence/computational_sequence.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, resource, destination, validate)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;31m#initializing the featureset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m#BACKWARD: backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/computational_sequence/computational_sequence.py\u001b[0m in \u001b[0;36m__initialize\u001b[0;34m(self, resource, destination, validate)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;31m#reading from url or folder - main_file is where the data should go and resource is the url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__initialize_from_csd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/computational_sequence/computational_sequence.py\u001b[0m in \u001b[0;36m__initialize_from_csd\u001b[0;34m(self, resource, destination, validate)\u001b[0m\n\u001b[1;32m     74\u001b[0m                                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination needs to be a folder where the downloaded computational sequence is stored. Exiting ...!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                         \u001b[0mread_URL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/computational_sequence/download_ops.py\u001b[0m in \u001b[0;36mread_URL\u001b[0;34m(url, destination)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s file already exists ...\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/CMU-MultimodalSDK/mmsdk/mmdatasdk/log/log.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(msgstring, error, errorType, destination, verbose)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mbcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBOLD\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"[%s] | Error   | \"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsgstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merrorType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsgstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mdest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdestination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: cmumosi/CMU_MOSI_TimestampedWordVectors.csd file already exists ..."
          ]
        }
      ],
      "source": [
        "from mmsdk import mmdatasdk\n",
        "cmumosi_highlevel = mmdatasdk.mmdataset(mmdatasdk.cmu_mosi.highlevel, 'cmumosi/')\n",
        "cmumosi_highlevel.align('glove_vectors', collapse_functions=[myavg])\n",
        "cmumosi_highlevel.add_computational_sequences(mmdatasdk.cmu_mosi.labels, 'cmumosi/')\n",
        "cmumosi_highlevel.align('Opinion Segment Labels')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "U6vAfAJxLHd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f08b1c7-8660-4413-b984-b6faf99ebbd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mmsdk in /usr/local/lib/python3.11/dist-packages (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from mmsdk) (3.12.1)\n",
            "Requirement already satisfied: validators>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from mmsdk) (0.34.0)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from mmsdk) (4.67.1)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from mmsdk) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "pip install mmsdk numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "p6GfR4mLStHG"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "def myavg(intervals, features):\n",
        "    return numpy.average(features, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im2TSkxjTxUk"
      },
      "source": [
        "#Model Design\n",
        "#Text Encoder:\n",
        "\n",
        "Use a BERT-based encoder for textual features.\n",
        "Extract contextual embeddings for the sentences.\n",
        "#Visual Encoder:\n",
        "\n",
        "Apply a fully connected (FC) network or CNN to process visual features.\n",
        "Reduce dimensionality for attention compatibility.\n",
        "#Attention-Based Fusion:\n",
        "\n",
        "Use a dot-product attention mechanism to combine textual and visual features.\n",
        "Compute attention weights to emphasize relevant features from both modalities.\n",
        "#Classification Head:\n",
        "\n",
        "Concatenate the fused representation.\n",
        "Pass it through dense layers for emotion classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOY0uzl-UAfc"
      },
      "source": [
        "**Code for the Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-q6jVj7LUEnw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "class MultimodalEmotionClassifier(nn.Module):\n",
        "    def __init__(self, text_model_name='bert-base-uncased', visual_dim=512, hidden_dim=256, num_classes=7):\n",
        "        super(MultimodalEmotionClassifier, self).__init__()\n",
        "\n",
        "        # Text encoder (BERT)\n",
        "        self.text_encoder = BertModel.from_pretrained(text_model_name)\n",
        "        self.text_fc = nn.Linear(self.text_encoder.config.hidden_size, hidden_dim)\n",
        "\n",
        "        # Visual encoder\n",
        "        self.visual_fc = nn.Linear(visual_dim, hidden_dim)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=1, batch_first=True)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, text_input_ids, text_attention_mask, visual_features):\n",
        "        # Textual features\n",
        "        text_outputs = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_embeds = self.text_fc(text_outputs.pooler_output)  # [Batch, Hidden]\n",
        "\n",
        "        # Visual features\n",
        "        visual_embeds = self.visual_fc(visual_features)  # [Batch, Hidden]\n",
        "\n",
        "        # Attention-based fusion\n",
        "        fused_embeds, _ = self.attention(\n",
        "            query=text_embeds.unsqueeze(1),\n",
        "            key=visual_embeds.unsqueeze(1),\n",
        "            value=visual_embeds.unsqueeze(1)\n",
        "        )\n",
        "        fused_embeds = fused_embeds.squeeze(1)  # [Batch, Hidden]\n",
        "\n",
        "        # Concatenate text and fused visual features\n",
        "        combined_features = torch.cat([text_embeds, fused_embeds], dim=-1)\n",
        "\n",
        "        # Classification\n",
        "        outputs = self.classifier(combined_features)\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRCzEBXVUSjM"
      },
      "source": [
        "**Training and Evaluation Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p__AxZ6uUVgP"
      },
      "source": [
        "**1. Training and Evaluation Setup**\n",
        "**Loss Function**\n",
        "Use CrossEntropyLoss since the task is multi-class classification.\n",
        "**Optimizer**\n",
        "Adam optimizer with weight decay for regularization.\n",
        "**Evaluation Metrics**\n",
        "Accuracy\n",
        "F1-score (micro and macro)\n",
        "**2. Training Loop Implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ZmlKHFG_UgS1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, device, num_epochs=5):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for batch in train_loader:\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(text_input_ids=text_ids, text_attention_mask=(text_ids != 0), visual_features=visual_features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate metrics\n",
        "            epoch_loss += loss.item()\n",
        "            preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics for the epoch\n",
        "        epoch_accuracy = accuracy_score(all_labels, all_preds)\n",
        "        epoch_f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        epoch_f1_micro = f1_score(all_labels, all_preds, average=\"micro\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {epoch_loss:.4f}, \"\n",
        "              f\"Accuracy: {epoch_accuracy:.4f}, F1-Macro: {epoch_f1_macro:.4f}, F1-Micro: {epoch_f1_micro:.4f}\")\n",
        "\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(text_input_ids=text_ids, text_attention_mask=(text_ids != 0), visual_features=visual_features)\n",
        "            preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "    f1_micro = f1_score(all_labels, all_preds, average=\"micro\")\n",
        "\n",
        "    print(f\"Validation - Accuracy: {accuracy:.4f}, F1-Macro: {f1_macro:.4f}, F1-Micro: {f1_micro:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqFXahkjUi6s"
      },
      "source": [
        "**3.Training Example**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "6S94vPhqUzUs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "c6a2218b-0827-4089-fed1-e84c173b3fba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-f1546f93a6ec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "# Initialize model, criterion, optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MultimodalEmotionClassifier()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Training\n",
        "train_model(model, train_loader, criterion, optimizer, device, num_epochs=10)\n",
        "\n",
        "# Evaluation\n",
        "evaluate_model(model, val_loader, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3UI7RufVAEI"
      },
      "source": [
        "**Stage 4: Visualizing the Attention Mechanism**\n",
        "We will focus on **generating attention heatmaps** to visualize which parts of the textual and visual inputs the model focuses on for emotion classification.\n",
        "\n",
        "**1. Extracting Attention Weights**\n",
        "To visualize the attention mechanism, we need to:\n",
        "\n",
        "Access the attention weights from the forward pass of the model.\n",
        "Normalize the weights for better interpretability.\n",
        "Modify the MultimodalEmotionClassifier class to **return attention weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JD3KTGOnVM9a"
      },
      "outputs": [],
      "source": [
        "class MultimodalEmotionClassifier(nn.Module):\n",
        "    def __init__(self, text_hidden_size=768, visual_hidden_size=512, fusion_hidden_size=512, num_classes=7):\n",
        "        super(MultimodalEmotionClassifier, self).__init__()\n",
        "        self.text_encoder = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "        self.visual_encoder = nn.Linear(2048, visual_hidden_size)  # Assuming visual features are 2048-D\n",
        "\n",
        "        self.attention = nn.Linear(text_hidden_size + visual_hidden_size, 1)\n",
        "        self.fusion_layer = nn.Linear(text_hidden_size + visual_hidden_size, fusion_hidden_size)\n",
        "        self.output_layer = nn.Linear(fusion_hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, text_input_ids, text_attention_mask, visual_features):\n",
        "        # Text features\n",
        "        text_outputs = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_cls = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Visual features\n",
        "        visual_embeds = self.visual_encoder(visual_features)\n",
        "\n",
        "        # Concatenate and apply attention\n",
        "        concat_features = torch.cat([text_cls, visual_embeds], dim=1)\n",
        "        attention_weights = torch.softmax(self.attention(concat_features), dim=1)\n",
        "\n",
        "        # Weighted fusion\n",
        "        weighted_features = attention_weights * concat_features\n",
        "        fused_features = self.fusion_layer(weighted_features)\n",
        "        logits = self.output_layer(fused_features)\n",
        "\n",
        "        return logits, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cepa4mvOVQU0"
      },
      "source": [
        "**2. Visualizing Attention**\n",
        "We’ll extract and plot attention weights for a sample batch.\n",
        "\n",
        "**Code for Visualization:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hIJHDkUaVTUm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_attention(model, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, attention_weights = model(\n",
        "                text_input_ids=text_ids,\n",
        "                text_attention_mask=(text_ids != 0),\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            # Plot attention heatmap for first sample\n",
        "            attention_weights = attention_weights[0].cpu().numpy()\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.title(\"Attention Heatmap\")\n",
        "            plt.imshow(attention_weights.reshape(1, -1), cmap=\"viridis\", aspect=\"auto\")\n",
        "            plt.colorbar(label=\"Attention Weight\")\n",
        "            plt.xlabel(\"Feature Index\")\n",
        "            plt.ylabel(\"Attention\")\n",
        "            plt.show()\n",
        "\n",
        "            # Visualize for one batch only\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb630VCKVV7N"
      },
      "source": [
        "**3. Fine-Tuning Hyperparameters**\n",
        "\n",
        "**Key Hyperparameters to Tune: **\n",
        "\n",
        "**Learning rate:** Experiment with 1e-4, 5e-5, and 1e-5.\n",
        "**Batch size:** Try 16, 32, and 64.\n",
        "**Fusion hidden size:** Adjust between 256 and 1024.\n",
        "\n",
        "**Implementation:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8oaRsA5LVmlp"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_tuning():\n",
        "    learning_rates = [1e-4, 5e-5, 1e-5]\n",
        "    batch_sizes = [16, 32, 64]\n",
        "    fusion_hidden_sizes = [256, 512, 1024]\n",
        "\n",
        "    best_accuracy = 0\n",
        "    best_config = None\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        for bs in batch_sizes:\n",
        "            for fs in fusion_hidden_sizes:\n",
        "                model = MultimodalEmotionClassifier(fusion_hidden_size=fs)\n",
        "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "                # Update DataLoader batch size\n",
        "                train_loader = DataLoader(train_dataset, batch_size=bs, collate_fn=multi_collate)\n",
        "\n",
        "                train_model(model, train_loader, criterion, optimizer, device, num_epochs=5)\n",
        "                val_accuracy = evaluate_model(model, val_loader, device)\n",
        "\n",
        "                if val_accuracy > best_accuracy:\n",
        "                    best_accuracy = val_accuracy\n",
        "                    best_config = {\"lr\": lr, \"batch_size\": bs, \"fusion_hidden_size\": fs}\n",
        "\n",
        "    print(f\"Best Configuration: {best_config}, Accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQLRohqSV54N"
      },
      "source": [
        "**4. Testing on Hold-Out Dataset**\n",
        "After identifying the best hyperparameters, evaluate the model on the **test set:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6aBVXT0JV-Ku",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "ee10a0cd-53b0-4c82-bd31-ab4b8bb75c5f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'best_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-544948f62427>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Accuracy: {test_accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
          ]
        }
      ],
      "source": [
        "test_accuracy = evaluate_model(best_model, test_loader, device)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLNZDlhDZrpQ"
      },
      "source": [
        "**Stage 5: Adding Interpretability Explanations**\n",
        "To enhance the model's interpretability, we’ll implement:\n",
        "\n",
        "**Feature Importance Analysis:** Show the relative contributions of textual and visual features.\n",
        "**Attention Interpretability:** Generate detailed heatmaps and overlays for specific examples.\n",
        "**1. Feature Importance Analysis**\n",
        "By analyzing the attention weights, we can calculate how much importance the model assigns to textual versus visual features.\n",
        "\n",
        "**Updated Model Forward Pass**\n",
        "In the model, attention weights are calculated over concatenated textual and visual features. To calculate their relative importance, separate and normalize the contributions of each modality:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "j4mhgPnGZzQq"
      },
      "outputs": [],
      "source": [
        "class MultimodalEmotionClassifier(nn.Module):\n",
        "    # Previous implementation...\n",
        "    def forward(self, text_input_ids, text_attention_mask, visual_features):\n",
        "        # Text features\n",
        "        text_outputs = self.text_encoder(input_ids=text_input_ids, attention_mask=text_attention_mask)\n",
        "        text_cls = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Visual features\n",
        "        visual_embeds = self.visual_encoder(visual_features)\n",
        "\n",
        "        # Concatenate and apply attention\n",
        "        concat_features = torch.cat([text_cls, visual_embeds], dim=1)\n",
        "        attention_weights = torch.softmax(self.attention(concat_features), dim=1)\n",
        "\n",
        "        # Calculate modality-specific importance\n",
        "        text_weights = attention_weights[:, :text_cls.size(1)].sum(dim=1)  # Text importance\n",
        "        visual_weights = attention_weights[:, text_cls.size(1):].sum(dim=1)  # Visual importance\n",
        "\n",
        "        # Weighted fusion\n",
        "        weighted_features = attention_weights * concat_features\n",
        "        fused_features = self.fusion_layer(weighted_features)\n",
        "        logits = self.output_layer(fused_features)\n",
        "\n",
        "        return logits, text_weights, visual_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-3cg6GqZ1_t"
      },
      "source": [
        "**Visualization of Feature Importance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uZu9Aqz3Z42U"
      },
      "outputs": [],
      "source": [
        "def visualize_feature_importance(model, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, text_weights, visual_weights = model(\n",
        "                text_input_ids=text_ids,\n",
        "                text_attention_mask=(text_ids != 0),\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            # Normalize and display importance\n",
        "            text_weights = text_weights.cpu().numpy()\n",
        "            visual_weights = visual_weights.cpu().numpy()\n",
        "\n",
        "            print(f\"Text Contribution: {text_weights.mean():.4f}\")\n",
        "            print(f\"Visual Contribution: {visual_weights.mean():.4f}\")\n",
        "\n",
        "            plt.bar([\"Text\", \"Visual\"], [text_weights.mean(), visual_weights.mean()])\n",
        "            plt.title(\"Feature Contribution\")\n",
        "            plt.ylabel(\"Importance\")\n",
        "            plt.show()\n",
        "\n",
        "            break  # Visualize for one batch only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFC78y7tZ7h_"
      },
      "source": [
        "**2. Attention Heatmaps with Overlays**\n",
        "For detailed interpretability, we’ll overlay attention heatmaps onto textual tokens or visual frames.\n",
        "\n",
        "**Visualizing Text Attention**\n",
        "Attention weights for textual tokens can highlight the model’s focus on specific words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yDpRD3C2aDck"
      },
      "outputs": [],
      "source": [
        "def visualize_text_attention(model, tokenizer, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, attention_weights, _ = model(\n",
        "                text_input_ids=text_ids,\n",
        "                text_attention_mask=(text_ids != 0),\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            attention_weights = attention_weights[:, :text_ids.size(1)].cpu().numpy()\n",
        "            text_ids = text_ids.cpu().numpy()\n",
        "\n",
        "            # Decode tokens and visualize attention\n",
        "            for i, sentence in enumerate(text_ids):\n",
        "                tokens = tokenizer.convert_ids_to_tokens(sentence)\n",
        "                weights = attention_weights[i]\n",
        "\n",
        "                plt.figure(figsize=(10, 2))\n",
        "                plt.bar(range(len(tokens)), weights, alpha=0.7)\n",
        "                plt.xticks(range(len(tokens)), tokens, rotation=90)\n",
        "                plt.title(\"Text Attention Heatmap\")\n",
        "                plt.ylabel(\"Attention Weight\")\n",
        "                plt.xlabel(\"Tokens\")\n",
        "                plt.show()\n",
        "\n",
        "            break  # Visualize for one batch only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkAxjGBMaGFQ"
      },
      "source": [
        "**Visualizing Visual Attention**\n",
        "For visual attention, overlay weights on frames."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b7XZ1EHDaIjs"
      },
      "outputs": [],
      "source": [
        "def visualize_visual_attention(model, dataloader, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            _, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, _, visual_weights = model(\n",
        "                text_input_ids=None,\n",
        "                text_attention_mask=None,\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            visual_weights = visual_weights.cpu().numpy()\n",
        "\n",
        "            for i, frame_weights in enumerate(visual_weights):\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                plt.title(f\"Frame {i} Attention Heatmap\")\n",
        "                plt.bar(range(len(frame_weights)), frame_weights, alpha=0.7)\n",
        "                plt.ylabel(\"Attention Weight\")\n",
        "                plt.xlabel(\"Frame Index\")\n",
        "                plt.show()\n",
        "\n",
        "            break  # Visualize for one batch only\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTSqtjUQaN8H"
      },
      "source": [
        "**3. Optimize Computation**\n",
        "For larger datasets or longer sequences:\n",
        "\n",
        "**Gradient Accumulation:** Reduce memory usage by splitting batches across steps.\n",
        "**Mixed Precision Training:** Use PyTorch’s AMP (torch.cuda.amp) for faster computation.\n",
        "**Token Truncation**: Limit text sequence length to reduce computation.\n",
        "\n",
        "**Example: Mixed Precision Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "sLEGufw7aaaW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "681fe6e5-930b-4930-e266-9b023438c426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-1ea617203157>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'num_epochs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-1ea617203157>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_epochs' is not defined"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            logits, _, _ = model(\n",
        "                text_input_ids=batch[0].to(device),\n",
        "                text_attention_mask=(batch[0] != 0).to(device),\n",
        "                visual_features=batch[1].to(device)\n",
        "            )\n",
        "            loss = criterion(logits, batch[3].to(device))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS4ia_Deav96"
      },
      "source": [
        "**Stage 6: Logging for Explainability Insights**\n",
        "In this stage, we’ll enhance the explainability by systematically logging intermediate results like attention scores and feature contributions. These logs will be saved in a structured format (e.g., JSON or CSV) for later analysis.\n",
        "\n",
        "**1. Logging Framework**\n",
        "We’ll use Python’s logging module to capture and save explainability metrics.\n",
        "\n",
        "**Set Up Logging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VMx1z1lua0XU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create a directory for logs\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"logs/explainability_logs.json\",\n",
        "    level=logging.INFO,\n",
        "    format='%(message)s'\n",
        ")\n",
        "\n",
        "def log_attention_scores(sample_id, text_tokens, text_weights, visual_weights):\n",
        "    log_data = {\n",
        "        \"sample_id\": sample_id,\n",
        "        \"text_tokens\": text_tokens,\n",
        "        \"text_weights\": text_weights.tolist(),\n",
        "        \"visual_weights\": visual_weights.tolist(),\n",
        "    }\n",
        "    logging.info(json.dumps(log_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLBMkieRa6LN"
      },
      "source": [
        "**Integration with Model**\n",
        "\n",
        "The model will now log text tokens, their attention weights, and visual frame weights during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EualPmnpa9Fm"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_log_explainability(model, dataloader, tokenizer, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sample_id, batch in enumerate(dataloader):\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            _, text_weights, visual_weights = model(\n",
        "                text_input_ids=text_ids,\n",
        "                text_attention_mask=(text_ids != 0),\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            # Decode tokens\n",
        "            text_ids = text_ids.cpu().numpy()\n",
        "            text_weights = text_weights.cpu().numpy()\n",
        "            visual_weights = visual_weights.cpu().numpy()\n",
        "\n",
        "            for i, sentence in enumerate(text_ids):\n",
        "                tokens = tokenizer.convert_ids_to_tokens(sentence)\n",
        "                log_attention_scores(\n",
        "                    sample_id=f\"{sample_id}_{i}\",\n",
        "                    text_tokens=tokens,\n",
        "                    text_weights=text_weights[i],\n",
        "                    visual_weights=visual_weights[i]\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Epl5RlZ3a_zY"
      },
      "source": [
        "**2. Packaging the Framework**\n",
        "\n",
        "We’ll organize the project into a modular structure for easier usage and reusability.\n",
        "\n",
        "**Project Structure**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "55gkSf89bFpI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "bf8d2ae6-928b-4833-8524-7cec0ed4ea09"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid character '├' (U+251C) (<ipython-input-28-bfb3ef11e69b>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-28-bfb3ef11e69b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    ├── data_processing/\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '├' (U+251C)\n"
          ]
        }
      ],
      "source": [
        "multimodal_emotion_analysis/\n",
        "├── data_processing/\n",
        "│   ├── mosi_dataset.py       # Dataset preprocessing\n",
        "├── models/\n",
        "│   ├── attention_model.py    # Model architecture\n",
        "├── training/\n",
        "│   ├── train.py              # Training logic\n",
        "│   ├── evaluation.py         # Evaluation and explainability logging\n",
        "├── visualization/\n",
        "│   ├── attention_visuals.py  # Heatmap visualizations\n",
        "├── utils/\n",
        "│   ├── logger.py             # Logging utilities\n",
        "│   ├── config.py             # Configurations\n",
        "├── main.py                   # Entry point\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGS4zEDibIh2"
      },
      "source": [
        "**Example: logger.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ExcZOmu3bKwu"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Create a directory for logs\n",
        "os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename=\"logs/explainability_logs.json\",\n",
        "    level=logging.INFO,\n",
        "    format='%(message)s'\n",
        ")\n",
        "\n",
        "def log_attention_scores(sample_id, text_tokens, text_weights, visual_weights):\n",
        "    log_data = {\n",
        "        \"sample_id\": sample_id,\n",
        "        \"text_tokens\": text_tokens,\n",
        "        \"text_weights\": text_weights.tolist(),\n",
        "        \"visual_weights\": visual_weights.tolist(),\n",
        "    }\n",
        "    logging.info(json.dumps(log_data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGiJh6fNbNwQ"
      },
      "source": [
        "**Example: train.py**\n",
        "\n",
        "This script will include training logic, integrating mixed precision and gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9jjET5jQbRbC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810a9259-a774-4fd4-b7d9-985b5401e078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-57a20b91c559>:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ],
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                logits, _, _ = model(\n",
        "                    text_input_ids=batch[0].to(device),\n",
        "                    text_attention_mask=(batch[0] != 0).to(device),\n",
        "                    visual_features=batch[1].to(device)\n",
        "                )\n",
        "                loss = criterion(logits, batch[3].to(device))\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Grq7gVW6bW7s"
      },
      "source": [
        "\n",
        "# **Stage 7: Visualizing Attention Mechanism (Heatmaps) **\n",
        "\n",
        "This stage focuses on generating attention heatmaps for explainability. The visualizations will display the importance of different words (textual features) and frames (visual features) for the model's decision.\n",
        "\n",
        "**1. Visualization Utility**\n",
        "We’ll use Matplotlib and Seaborn to create heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "UW7qJRz6bhKB"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "def plot_attention_heatmap(tokens, weights, title=\"Attention Heatmap\", save_path=None):\n",
        "    \"\"\"\n",
        "    Plot a heatmap for attention scores.\n",
        "\n",
        "    Args:\n",
        "        tokens (list): List of text tokens.\n",
        "        weights (list or np.array): Corresponding attention weights.\n",
        "        title (str): Title of the heatmap.\n",
        "        save_path (str): Path to save the heatmap image (optional).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.heatmap(\n",
        "        np.expand_dims(weights, axis=0),\n",
        "        annot=np.array(tokens).reshape(1, -1),\n",
        "        fmt=\"\",\n",
        "        cmap=\"Blues\",\n",
        "        cbar=False,\n",
        "        xticklabels=False,\n",
        "        yticklabels=False,\n",
        "    )\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.xlabel(\"Tokens\", fontsize=12)\n",
        "    plt.ylabel(\"Attention\", fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mEHgS8cbllP"
      },
      "source": [
        "**2. Generate Heatmaps for Evaluation Results**\n",
        "\n",
        "We’ll visualize the attention scores for text and visual modalities during evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Ziou1sGUblLz"
      },
      "outputs": [],
      "source": [
        "def visualize_attention_samples(model, dataloader, tokenizer, device, save_dir=\"heatmaps/\"):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sample_id, batch in enumerate(dataloader):\n",
        "            text_ids, visual_features, _, labels, _ = batch\n",
        "\n",
        "            # Move to device\n",
        "            text_ids = text_ids.to(device)\n",
        "            visual_features = visual_features.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, text_weights, visual_weights = model(\n",
        "                text_input_ids=text_ids,\n",
        "                text_attention_mask=(text_ids != 0),\n",
        "                visual_features=visual_features\n",
        "            )\n",
        "\n",
        "            # Decode tokens and visualize attention\n",
        "            text_ids = text_ids.cpu().numpy()\n",
        "            text_weights = text_weights.cpu().numpy()\n",
        "            visual_weights = visual_weights.cpu().numpy()\n",
        "\n",
        "            for i, sentence in enumerate(text_ids):\n",
        "                tokens = tokenizer.convert_ids_to_tokens(sentence)\n",
        "                text_attention = text_weights[i]\n",
        "                visual_attention = visual_weights[i]\n",
        "\n",
        "                # Plot text attention\n",
        "                plot_attention_heatmap(\n",
        "                    tokens=tokens,\n",
        "                    weights=text_attention,\n",
        "                    title=f\"Sample {sample_id}_{i} Text Attention\",\n",
        "                    save_path=f\"{save_dir}/text_sample_{sample_id}_{i}.png\"\n",
        "                )\n",
        "\n",
        "                # Plot visual attention (frames)\n",
        "                plot_attention_heatmap(\n",
        "                    tokens=[f\"Frame {j}\" for j in range(len(visual_attention))],\n",
        "                    weights=visual_attention,\n",
        "                    title=f\"Sample {sample_id}_{i} Visual Attention\",\n",
        "                    save_path=f\"{save_dir}/visual_sample_{sample_id}_{i}.png\"\n",
        "                )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def plot_attention_heatmap(tokens, weights, title, save_path=None):\n",
        "    plt.figure(figsize=(10, 1))  # تنظیم اندازه نمودار\n",
        "    sns.heatmap(\n",
        "        np.array([weights]),  # وزن‌ها به شکل آرایه دوبعدی\n",
        "        annot=False,  # عدم نمایش اعداد روی نقشه\n",
        "        cmap='YlGnBu',  # رنگ‌بندی\n",
        "        xticklabels=tokens,  # نمایش توکن‌ها به‌عنوان برچسب محور X\n",
        "        yticklabels=[]  # عدم نمایش برچسب محور Y\n",
        "    )\n",
        "    plt.xticks(rotation=90)  # چرخش برچسب‌های محور X\n",
        "    plt.title(title)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "E7QKFir3qvwX"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAYREhwfbuaZ"
      },
      "source": [
        "**Stage 8: Building main.py**\n",
        "\n",
        "This script will integrate preprocessing, training, evaluation, and visualization into a single pipeline.\n",
        "\n",
        "**Example Code for main.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "n4Qee5TLby4w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "2d35aca2-9dc3-4c1d-b53c-e335f4c810cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'data_processing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-f05c8b9467c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_processing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmosi_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMOSIDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultimodalAttentionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_processing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import BertTokenizer\n",
        "from data_processing.mosi_dataset import MOSIDataset\n",
        "from models.attention_model import MultimodalAttentionModel\n",
        "from training.train import train_model\n",
        "from training.evaluation import evaluate_and_log_explainability\n",
        "from visualization.attention_visuals import visualize_attention_samples\n",
        "\n",
        "def main():\n",
        "    # Configurations\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    batch_size = 32\n",
        "    num_epochs = 5\n",
        "    learning_rate = 1e-4\n",
        "\n",
        "    # Set your dataset path\n",
        "    #train_data_path = '/path/to/processed/train_data'\n",
        "    #val_data_path = '/path/to/processed/val_data'\n",
        "\n",
        "    # Load tokenizer and dataset\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    train_dataset = MOSIDataset(\"path_to_train_data\", tokenizer)\n",
        "    val_dataset = MOSIDataset(\"path_to_val_data\", tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)\n",
        "\n",
        "    # Initialize model\n",
        "    model = MultimodalAttentionModel()\n",
        "    model.to(device)\n",
        "\n",
        "    # Define optimizer and loss\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    # Train the model\n",
        "    train_model(model, train_loader, criterion, optimizer, num_epochs, device)\n",
        "\n",
        "    # Evaluate and log explainability\n",
        "    evaluate_and_log_explainability(model, val_loader, tokenizer, device)\n",
        "\n",
        "    # Visualize attention\n",
        "    visualize_attention_samples(model, val_loader, tokenizer, device)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVxyUzF0b1Aj"
      },
      "source": [
        "Stage 9: Packaging the Project\n",
        "**bold text**\n",
        "We’ll create a setup.py to package the project for reuse.\n",
        "\n",
        "**Example: setup.py**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "lwPeNnayb5JC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8753776f-6ed4-4d8b-ed9c-ac403d3f2495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/fancy_getopt.py\", line 245, in getopt\n",
            "    opts, args = getopt.getopt(args, short_opts, self.long_opts)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/getopt.py\", line 95, in getopt\n",
            "    opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/getopt.py\", line 195, in do_shorts\n",
            "    if short_has_arg(opt, shortopts):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/getopt.py\", line 211, in short_has_arg\n",
            "    raise GetoptError(_('option -%s not recognized') % opt, opt)\n",
            "getopt.GetoptError: option -f not recognized\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 170, in setup\n",
            "    ok = dist.parse_command_line()\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\", line 464, in parse_command_line\n",
            "    args = parser.getopt(args=self.script_args, object=self)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/fancy_getopt.py\", line 247, in getopt\n",
            "    raise DistutilsArgError(msg)\n",
            "distutils.errors.DistutilsArgError: option -f not recognized\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-34-66016578c39a>\", line 3, in <cell line: 0>\n",
            "    setup(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\", line 117, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\", line 172, in setup\n",
            "    raise SystemExit(gen_usage(dist.script_name) + f\"\\nerror: {msg}\")\n",
            "SystemExit: usage: colab_kernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
            "   or: colab_kernel_launcher.py --help [cmd1 cmd2 ...]\n",
            "   or: colab_kernel_launcher.py --help-commands\n",
            "   or: colab_kernel_launcher.py cmd --help\n",
            "\n",
            "error: option -f not recognized\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1671, in getframeinfo\n",
            "    lineno = frame.f_lineno\n",
            "             ^^^^^^^^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'f_lineno'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/fancy_getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[0;34m(self, args, object)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_opts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[0;34m(args, shortopts, longopts)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_shorts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/getopt.py\u001b[0m in \u001b[0;36mdo_shorts\u001b[0;34m(opts, optstring, shortopts, args)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptstring\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mshort_has_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moptstring\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/getopt.py\u001b[0m in \u001b[0;36mshort_has_arg\u001b[0;34m(opt, shortopts)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mGetoptError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'option -%s not recognized'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGetoptError\u001b[0m: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mDistutilsArgError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_command_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDistutilsArgError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/dist.py\u001b[0m in \u001b[0;36mparse_command_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aliases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'licence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'license'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetopt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0moption_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_option_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/fancy_getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[0;34m(self, args, object)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mDistutilsArgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDistutilsArgError\u001b[0m: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-66016578c39a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m setup(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"multimodal_emotion_analysis\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/__init__.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0m_install_setup_requires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/core.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(**attrs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mDistutilsArgError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscript_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\"\\nerror: {msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: usage: colab_kernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: colab_kernel_launcher.py --help [cmd1 cmd2 ...]\n   or: colab_kernel_launcher.py --help-commands\n   or: colab_kernel_launcher.py cmd --help\n\nerror: option -f not recognized",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "from setuptools import setup, find_packages\n",
        "\n",
        "setup(\n",
        "    name=\"multimodal_emotion_analysis\",\n",
        "    version=\"1.0\",\n",
        "    packages=find_packages(),\n",
        "    install_requires=[\n",
        "        \"torch\",\n",
        "        \"transformers\",\n",
        "        \"numpy\",\n",
        "        \"matplotlib\",\n",
        "        \"seaborn\",\n",
        "    ],\n",
        "    entry_points={\n",
        "        \"console_scripts\": [\n",
        "            \"emotion-analysis=main:main\",\n",
        "        ],\n",
        "    },\n",
        "    description=\"Multimodal Emotion Analysis with Attention Mechanism\",\n",
        "    author=\"Your Name\",\n",
        "    author_email=\"your_email@example.com\",\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJZKS+VBZTQetcwVFhAHcC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}